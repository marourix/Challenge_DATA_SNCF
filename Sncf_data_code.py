#Importation des BibliothÃ¨ques
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.linear_model import ElasticNet
from sklearn.svm import SVR
from lightgbm import LGBMRegressor
from xgboost import XGBRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import RobustScaler
#Chargement de la data
dfx=pd.read_csv('/content/x_train_final.csv')
dfy=pd.read_csv('/content/y_train_final_j5KGWWK.csv')
print(dfx.shape)
print(dfy.shape)
print(dfx.head())
print(dfy.head())
dfx=dfx.drop('Unnamed: 0.1',axis=1)
dfx=dfx.drop('Unnamed: 0',axis=1)
dfy=dfy.drop('Unnamed: 0',axis=1)
#Exploration des DonnÃ©es (EDA)
print(dfx.info())
print(dfx.describe())
# 2. Distribution de la cible
plt.figure(figsize=(8, 4))
sns.histplot(dfy['p0q0'], kde=True, bins=30)
plt.title("Distribution de la cible p0q0 (AVANT traitement)")
plt.xlabel("p0q0")
plt.ylabel("FrÃ©quence")
plt.show()
#concatÃ©nation des nos variable avec le label pour faire le nettoyage
df_concat = pd.concat([dfx, dfy], axis=1)
# 3. Boxplot sur quelques colonnes numÃ©riques
numeric_cols = ['p0q0', 'p2q0', 'p3q0', 'p4q0', 'p0q2', 'p0q3', 'p0q4']

plt.figure(figsize=(12, 5))
sns.boxplot(data=df_concat[numeric_cols])
plt.xticks(rotation=45)
plt.title("Boxplot des variables numÃ©riques AVANT traitement des outliers")
plt.show()
train=dfx["train"].value_counts()
gare=dfx["gare"].value_counts()
date=dfx["date"].value_counts()
print(train)
print(gare)
print(date)
#Traitement des valeur aberrantes
df_concat.head()
df_concat.describe()
print(df_concat.describe(include='object'))
df_concat.isna().sum()
def remove_outliers_iqr(df):
    """
    Supprime les outliers des colonnes numÃ©riques du DataFrame en utilisant la mÃ©thode IQR.
    """
    df_clean = df.copy()  # Copier le DataFrame pour Ã©viter les modifications en place
    for col in df.select_dtypes(include=['number']).columns:  # Appliquer aux colonnes numÃ©riques
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        # Filtrer les valeurs dans l'intervalle acceptable
        df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]

    return df_clean

# Application de la fonction
df_cleaned = remove_outliers_iqr(df_concat)
df_cleaned.describe()
df_concat.describe()
#EDA APRÃˆS le traitement des outliers
print("ðŸ“Š Structure des donnÃ©es APRÃˆS traitement des outliers")
print(df_cleaned.info())
print(df_cleaned.describe())

# Distribution de la cible aprÃ¨s traitement
plt.figure(figsize=(8, 4))
sns.histplot(df_cleaned['p0q0'], kde=True, bins=30)
plt.title("Distribution de la cible p0q0 (APRÃˆS traitement)")
plt.xlabel("p0q0")
plt.ylabel("FrÃ©quence")
plt.show()

# Boxplot des colonnes numÃ©riques aprÃ¨s nettoyage
plt.figure(figsize=(12, 5))
sns.boxplot(data=df_cleaned[numeric_cols])
plt.xticks(rotation=45)
plt.title("Boxplot des variables numÃ©riques APRÃˆS traitement des outliers")
plt.show()
# SÃ©lection des colonnes numÃ©riques
numeric_features = df_cleaned.select_dtypes(include='number')

# Matrice de corrÃ©lation
plt.figure(figsize=(12, 10))
sns.heatmap(numeric_features.corr(), annot=True, fmt=".2f", cmap="coolwarm", square=True)
plt.title(" Matrice de corrÃ©lation APRÃˆS suppression des outliers")
plt.show()
#Visualisation des donnÃ©es
# Histogramme pour voir la distribution aprÃ¨s nettoyage
df_cleaned.hist(figsize=(12, 8), bins=30)
plt.suptitle("Distribution des variables aprÃ¨s suppression des outliers")
plt.show()
#PrÃ©paration des DonnÃ©es
# Conversion de la colonne en type datetime
df_cleaned['date'] = pd.to_datetime(df_cleaned['date'])

# Extraction du jour de la semaine (lundi=0, dimanche=6)
df_cleaned['jour_num'] = df_cleaned['date'].dt.dayofweek

# Encodage cyclique : transformation en coordonnÃ©es sin et cos
df_cleaned['jour_sin'] = np.sin(2 * np.pi * df_cleaned['jour_num'] / 7)
df_cleaned['jour_cos'] = np.cos(2 * np.pi * df_cleaned['jour_num'] / 7)

# Suppression de la colonne date si elle n'est plus utile
df_cleaned.drop(columns=['date', 'jour_num'], inplace=True)
df_cleaned.head()
# 'handle_unknown="ignore"' permet dâ€™Ã©viter les erreurs si des valeurs inconnues apparaissent en test.
# 'sparse_output=False' signifie que la sortie sera un tableau dense (de type numpy array), plus facile Ã  manipuler.
encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)


encoder.fit(df_cleaned[['gare']])

encoded_train = encoder.transform(df_cleaned[['gare']])

# CrÃ©ation dâ€™un DataFrame Ã  partir des donnÃ©es encodÃ©es, avec :
# - des noms de colonnes explicites (ex : "gare_Paris", "gare_Lyon", etc.)
# - les mÃªmes index que df_cleaned pour conserver lâ€™alignement des lignes.
encoded_train_df = pd.DataFrame(
    encoded_train,
    columns=[f"gare_{cat}" for cat in encoder.categories_[0]],
    index=df_cleaned.index
)

# Fusion du DataFrame original (sans la colonne 'gare') avec le DataFrame contenant les colonnes encodÃ©es.
df_cleaned = pd.concat([df_cleaned.drop(columns=['gare']), encoded_train_df], axis=1)
df_cleaned.head()
# Initialiser LabelEncoder
le = LabelEncoder()

# Appliquer Label Encoding aux colonnes 'train'
df_cleaned['train_encoded'] = le.fit_transform(df_cleaned['train'])
df_cleaned=df_cleaned.drop('train',axis=1)
df_cleaned.head()
df_cleaned.head()
"""SÃ©paration de donnÃ©es

On sÃ©pare le jeu de donnÃ©es en deux parties :

X contient toutes les variables explicatives
y contient la variable cible (p0q0)
80% des donnÃ©es seront utilisÃ©es pour l'entraÃ®nement (X_train, y_train)
20% seront rÃ©servÃ©es pour le test (X_test, y_test)"""
# On suppose que df_cleaned est dÃ©jÃ  chargÃ© et prÃªt Ã  l'emploi
dff = df_cleaned.copy()

# X = toutes les colonnes sauf 'p0q0'
X = dff.drop('p0q0', axis=1)
feature_names = X.columns.tolist() ## Enregistrement de la structure des colonnes utilisÃ©es pour l'entraÃ®nement
# y = colonne cible
y = dff['p0q0']
# 80% pour l'entraÃ®nement, 20% pour le test
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42  # pour la reproductibilitÃ©
)
scaler = RobustScaler()

# On suppose que toutes les colonnes de X sont numÃ©riques aprÃ¨s encodage (ce qui est le cas ici)
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
#RÃ©gression LinÃ©aire

# CrÃ©ation et entraÃ®nement du modÃ¨le de rÃ©gression linÃ©aire
model_lr = LinearRegression()
model_lr.fit(X_train, y_train)

# PrÃ©dictions sur le jeu de test
y_pred = model_lr.predict(X_test)

# Calcul des mÃ©triques : MAE et RÂ²
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("MAE (Mean Absolute Error) :", mae)
print("RÂ² Score :", r2)
#XGBOOST regressor
import optuna
from xgboost import XGBRegressor
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.metrics import mean_absolute_error, r2_score
import numpy as np
#DÃ©finition de l'objectif Optuna avec validation croisÃ©e
def objective(trial):
param = {
 'n_estimators': trial.suggest_int('n_estimators', 100, 500),
 'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),
 'max_depth': trial.suggest_int('max_depth', 3, 10),
 'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),
 'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),
 'reg_alpha': trial.suggest_loguniform('reg_alpha', 0.01, 1.0),
 'reg_lambda': trial.suggest_loguniform('reg_lambda', 0.01, 1.0),
   }
model = XGBRegressor(**param, random_state=42)
#Cross-validation sur 5 folds
scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)
return -scores.mean()  # Minimiser le MAE
#SÃ©parer les donnÃ©es en train et test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
#Optimisation avec Optuna
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=50)
#Meilleurs hyperparamÃ¨tres trouvÃ©s
best_params = study.best_params
print(f"Meilleurs hyperparamÃ¨tres Optuna : {best_params}")
#EntraÃ®ner le modÃ¨le final avec les meilleurs hyperparamÃ¨tres
best_model = XGBRegressor(**best_params, random_state=42)
best_model.fit(X_train, y_train)
#PrÃ©dictions finales
y_pred_best = best_model.predict(X_test)
#Ã‰valuation
mae_best = mean_absolute_error(y_test, y_pred_best)
r2_best = r2_score(y_test, y_pred_best)
print("ðŸ“Œ XGBoost avec Optuna et Cross Validation")
print("ðŸ”¹ MAE:", mae_best)
print("ðŸ”¹ RÂ² Score:", r2_best)
print("-" * 40)
